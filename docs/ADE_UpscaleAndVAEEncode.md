# Scale Ref Image and VAE Encode ğŸ­ğŸ…ğŸ…“â‘¡
## Documentation
- Class name: ADE_UpscaleAndVAEEncode
- Category: Animate Diff ğŸ­ğŸ…ğŸ…“/â‘¡ Gen2 nodes â‘¡/AnimateLCM-I2V
- Output node: False
- Repo Ref: https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved.git

ADE_UpscaleAndVAEEncodeèŠ‚ç‚¹æ—¨åœ¨é€šè¿‡é¦–å…ˆå°†å›¾åƒæ”¾å¤§åˆ°æ›´é«˜åˆ†è¾¨ç‡ï¼Œç„¶åä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å°†å…¶ç¼–ç ä¸ºæ½œåœ¨è¡¨ç¤ºæ¥å¤„ç†å›¾åƒã€‚æ­¤èŠ‚ç‚¹æ˜¯AnimateDiffå¥—ä»¶çš„ä¸€éƒ¨åˆ†ï¼Œä¸“é—¨ç”¨äºåœ¨åº”ç”¨è¿›ä¸€æ­¥çš„ç”Ÿæˆæˆ–è½¬æ¢è¿‡ç¨‹ä¹‹å‰å¢å¼ºå›¾åƒè´¨é‡ã€‚

## Input types
### Required
- image
    - è¡¨ç¤ºè¦æ”¾å¤§å’Œç¼–ç çš„è¾“å…¥å›¾åƒçš„å‚æ•°ã€‚å®ƒåœ¨ç¡®å®šæœ€ç»ˆæ½œåœ¨è¡¨ç¤ºçš„è´¨é‡å’Œåˆ†è¾¨ç‡æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚
    - Comfy dtype: IMAGE
    - Python dtype: torch.Tensor
- vae
    - æŒ‡å®šç”¨äºå°†æ”¾å¤§çš„å›¾åƒç¼–ç ä¸ºå…¶æ½œåœ¨è¡¨ç¤ºçš„å˜åˆ†è‡ªç¼–ç å™¨æ¨¡å‹ã€‚å®ƒå½±å“ç¼–ç æ•ˆç‡å’Œç”Ÿæˆçš„æ½œåœ¨ç©ºé—´çš„è´¨é‡ã€‚
    - Comfy dtype: VAE
    - Python dtype: torch.nn.Module
- latent_size
    - è¡¨ç¤ºè¦ç”Ÿæˆçš„æ½œåœ¨è¡¨ç¤ºçš„å¤§å°ã€‚å®ƒå†³å®šè¾“å‡ºæ½œåœ¨ç©ºé—´çš„ç»´åº¦ã€‚
    - Comfy dtype: LATENT
    - Python dtype: torch.Tensor
- scale_method
    - å®šä¹‰ç”¨äºæ”¾å¤§å›¾åƒçš„æ–¹æ³•ã€‚å®ƒå½±å“æ”¾å¤§å›¾åƒçš„è´¨é‡ã€‚
    - Comfy dtype: COMBO[STRING]
    - Python dtype: str
- crop
    - æŒ‡å®šæ”¾å¤§ååº”ç”¨çš„è£å‰ªæ–¹æ³•ï¼Œå½±å“æœ€ç»ˆå›¾åƒçš„æ„å›¾ã€‚
    - Comfy dtype: COMBO[STRING]
    - Python dtype: str

## Output types
- latent
    - Comfy dtype: LATENT
    - è¾“å‡ºæ˜¯è¾“å…¥å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºï¼Œç”±VAEåœ¨æ”¾å¤§åç¼–ç ã€‚å®ƒä»¥å‹ç¼©å½¢å¼æ•æ‰å›¾åƒçš„åŸºæœ¬ç‰¹å¾ï¼Œé€‚åˆè¿›ä¸€æ­¥çš„ç”Ÿæˆä»»åŠ¡ã€‚
    - Python dtype: Dict[str, torch.Tensor]

## Usage tips
- Infra type: GPU
- Common nodes: unknown

## Source code
```python
class UpscaleAndVaeEncode:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": ("IMAGE",),
                "vae": ("VAE",),
                "latent_size": ("LATENT",),
                "scale_method": (ScaleMethods._LIST_IMAGE,),
                "crop": (CropMethods._LIST, {"default": CropMethods.CENTER},),
            }
        }
    
    RETURN_TYPES = ("LATENT",)
    FUNCTION = "preprocess_images"

    CATEGORY = "Animate Diff ğŸ­ğŸ…ğŸ…“/â‘¡ Gen2 nodes â‘¡/AnimateLCM-I2V"

    def preprocess_images(self, image: torch.Tensor, vae: VAE, latent_size: torch.Tensor, scale_method: str, crop: str):
        b, c, h, w = latent_size["samples"].size()
        image = image.movedim(-1,1)
        image = comfy.utils.common_upscale(samples=image, width=w*8, height=h*8, upscale_method=scale_method, crop=crop)
        image = image.movedim(1,-1)
        # now that images are the expected size, VAEEncode them
        try:  # account for old ComfyUI versions (TODO: remove this when other changes require ComfyUI update)
            if not hasattr(vae, "vae_encode_crop_pixels"):
                image = VAEEncode.vae_encode_crop_pixels(image)
        except Exception:
            pass
        return ({"samples": vae.encode(image[:,:,:,:3])},)